{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math as mth\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15001, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df = df.loc[:15000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col = df.iloc[1:, 0]\n",
    "second_col = df.iloc[1:, 1]\n",
    "second_col = second_col.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = open(\"stopwords.txt\",\"r\",encoding=\"utf-8\")\n",
    "stop_words = stop_words_file.read()\n",
    "stop_words = stop_words.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning method\n",
    "def data_cleaning(string):\n",
    "    text = re.sub('\\,|\\@|\\-|\\\"|\\'| \\)|\\(|\\)| \\{| \\}| \\[| \\]|!|‘|’|“|”| \\:-|\\?|।|/|\\—|\\०|\\१|\\२|\\३|\\४|\\५|\\६|\\७|\\८|\\९|[0-9]', '', string)\n",
    "    return text\n",
    "\n",
    "def stop_word_remove(array_element):\n",
    "    array_element_set = set(array_element)\n",
    "    final_list = list(array_element_set.difference(stop_words))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_split = []\n",
    "def tokenize():\n",
    "    for data in first_col:\n",
    "        return_string = data_cleaning(data)\n",
    "        each_docs = return_string.split()\n",
    "        string_after_remove_word=stop_word_remove(each_docs)\n",
    "        # print(string_after_remove_word)\n",
    "        data_with_split.append(string_after_remove_word)\n",
    "    return data_with_split  # it returns arr of each docs with spleted words\n",
    "\n",
    "\n",
    "\n",
    "corpus = tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = None\n",
    "        self.idf = None\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        # Build vocabulary\n",
    "        self.vocabulary = set()\n",
    "        for document in corpus:\n",
    "            self.vocabulary.update(document)\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "\n",
    "        # Calculate IDF\n",
    "        idf = {}\n",
    "        N = len(corpus)\n",
    "        for term in self.vocabulary:\n",
    "            df = sum(1 for document in corpus if term in document)\n",
    "            idf[term] = math.log(N / (1 + df))\n",
    "\n",
    "        # Transform documents to TF-IDF representation\n",
    "        tfidf_matrix = np.zeros((len(corpus), len(self.vocabulary)))\n",
    "        for i, document in enumerate(corpus):\n",
    "            tf = Counter(document)\n",
    "            total_terms = len(document)\n",
    "            for j, term in enumerate(self.vocabulary):\n",
    "                if total_terms != 0:\n",
    "                    tfidf_matrix[i, j] = (tf.get(term, 0) / total_terms) * idf[term]\n",
    "                else:\n",
    "                    tfidf_matrix[i, j] = 0  # Set TF-IDF to 0 if total_terms is 0\n",
    "\n",
    "        self.idf = idf\n",
    "        return tfidf_matrix\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        tfidf_matrix = np.zeros((len(corpus), len(self.vocabulary)))\n",
    "        for i, document in enumerate(corpus):\n",
    "            tf = Counter(document)\n",
    "            total_terms = len(document)\n",
    "            for j, term in enumerate(self.vocabulary):\n",
    "                if total_terms != 0:\n",
    "                    tfidf_matrix[i, j] = (tf.get(term, 0) / total_terms) * self.idf.get(term, 0)\n",
    "                else:\n",
    "                    tfidf_matrix[i, j] = 0  # Set TF-IDF to 0 if total_terms is 0\n",
    "        return tfidf_matrix\n",
    "\n",
    "# Create TFIDFVectorizer instance\n",
    "tfidf_vectorizer = TFIDFVectorizer()\n",
    "\n",
    "# Fit and transform corpus\n",
    "features = tfidf_vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=features\n",
    "y=second_col\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2,random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()  \n",
    "TrainData = naive_bayes.fit(train_x, train_y)\n",
    "if __name__ == '__main__':\n",
    "    classifier_data = open(\"classify_data.pickle\", \"wb\")\n",
    "    pickle.dump(naive_bayes, classifier_data)\n",
    "    classifier_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classify_data.pickle', 'rb') as pickle_saved_data:\n",
    "    unpickled_data = pickle.load(pickle_saved_data)\n",
    "\n",
    "\n",
    "\n",
    "prediction = unpickled_data.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6789278569864095\n",
      "Recall: 0.6906666666666667\n",
      "Accuracy: 0.6906666666666667\n",
      "F1 Score: 0.6770261612248695\n"
     ]
    }
   ],
   "source": [
    "def calculate_performance_metrics(true_labels, predicted_labels):\n",
    "    precision = metrics.precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = metrics.recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    accuracy = metrics.accuracy_score(true_labels, predicted_labels)\n",
    "    f1_score = metrics.f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    return precision, recall, accuracy, f1_score\n",
    "\n",
    "# Example usage:\n",
    "precision, recall, accuracy, f1_score = calculate_performance_metrics(test_y, prediction)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Label: -1\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    # Preprocess the input sentence\n",
    "    cleaned_sentence = data_cleaning(sentence)\n",
    "    tokenized_sentence = stop_word_remove(cleaned_sentence.split())\n",
    "\n",
    "    # Transform the preprocessed sentence using TF-IDF vectorizer\n",
    "    sentence_features = tfidf_vectorizer.transform([tokenized_sentence])\n",
    "\n",
    "    # Use the trained classifier to predict the sentiment label\n",
    "    predicted_label = unpickled_data.predict(sentence_features)\n",
    "\n",
    "    return predicted_label[0]  # Return the predicted sentiment label\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"म नराम्रो केटा हुँ।\"\n",
    "predicted_sentiment = predict_sentiment(sentence)\n",
    "print(\"Predicted Sentiment Label:\", predicted_sentiment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
